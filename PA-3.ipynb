{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f44305df-56e1-44a1-9012-57e075f40aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8950\n",
      "Confusion Matrix:\n",
      "[[2922   78]\n",
      " [ 361  821]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# Initialize the stemmer for stemming version\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def load_data_from_files(directory):\n",
    "    \"\"\"\n",
    "    Load text files from a directory.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            data.append(file.read())\n",
    "    return data\n",
    "\n",
    "def preprocess_text(text, stem=False):\n",
    "    \"\"\"\n",
    "    Preprocesses text by tokenizing, handling punctuation, emoticons, and stemming if specified.\n",
    "    \"\"\"\n",
    "    # Tokenization pattern that handles words, punctuation, and emoticons\n",
    "    pattern = r\"\"\"(?x)                  \n",
    "                  (?:[A-Z]\\.)+            # Initials like U.S.A.\n",
    "                  |\\$?\\d+(?:\\.\\d+)?%?     # Currency/Percentages\n",
    "                  |\\w+(?:[-']\\w+)*        # Words with optional internal hyphens/apostrophes\n",
    "                  |[.,;\"'?():-_`]+        # Punctuation\n",
    "                  |[:;=8][\\-o\\*\\']?[)\\](\\d]  # Basic emoticons (like :) or :-))\n",
    "              \"\"\"\n",
    "    tokens = regexp_tokenize(text.lower(), pattern)\n",
    "    tokens = [re.sub(r'<.*?>', '', word) for word in tokens]  # Remove HTML tags\n",
    "    tokens = [stemmer.stem(word) if stem else word for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def build_vocabulary(data, stem=False):\n",
    "    \"\"\"\n",
    "    Build vocabulary from the training data.\n",
    "    \"\"\"\n",
    "    vocabulary = defaultdict(int)\n",
    "    for text in data:\n",
    "        tokens = preprocess_text(text, stem=stem)\n",
    "        for word in tokens:\n",
    "            vocabulary[word] += 1\n",
    "    return vocabulary\n",
    "\n",
    "# Load Data\n",
    "train_positive_data = load_data_from_files('/Users/saivaruntanjoreraghavendra/Downloads/tweet 2/train/positive')\n",
    "train_negative_data = load_data_from_files('/Users/saivaruntanjoreraghavendra/Downloads/tweet 2/train/negative')\n",
    "test_positive_data = load_data_from_files('/Users/saivaruntanjoreraghavendra/Downloads/tweet 2/test/positive')\n",
    "test_negative_data = load_data_from_files('/Users/saivaruntanjoreraghavendra/Downloads/tweet 2/test/negative')\n",
    "\n",
    "# Combine training data\n",
    "train_data = train_positive_data + train_negative_data\n",
    "train_labels = [1] * len(train_positive_data) + [0] * len(train_negative_data)\n",
    "test_data = test_positive_data + test_negative_data\n",
    "test_labels = [1] * len(test_positive_data) + [0] * len(test_negative_data)\n",
    "\n",
    "# Build Vocabulary (With and Without Stemming)\n",
    "vocabulary_without_stemming = build_vocabulary(train_data, stem=False)\n",
    "vocabulary_with_stemming = build_vocabulary(train_data, stem=True)\n",
    "\n",
    "# Convert Documents to Feature Vectors (Binary and Frequency)\n",
    "def document_to_vector(text, vocabulary, stem=False, binary=False):\n",
    "    tokens = preprocess_text(text, stem=stem)\n",
    "    word_counts = Counter(tokens)\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if word in vocabulary:\n",
    "            index = list(vocabulary.keys()).index(word)\n",
    "            vector[index] = 1 if binary else count\n",
    "    return vector\n",
    "\n",
    "# Convert all training and test data into vectors\n",
    "train_vectors_freq = [document_to_vector(text, vocabulary_without_stemming, stem=False, binary=False) for text in train_data]\n",
    "train_vectors_bin = [document_to_vector(text, vocabulary_without_stemming, stem=False, binary=True) for text in train_data]\n",
    "test_vectors_freq = [document_to_vector(text, vocabulary_without_stemming, stem=False, binary=False) for text in test_data]\n",
    "test_vectors_bin = [document_to_vector(text, vocabulary_without_stemming, stem=False, binary=True) for text in test_data]\n",
    "\n",
    "# Naive Bayes Model\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_word_counts = {0: defaultdict(int), 1: defaultdict(int)}\n",
    "        self.class_totals = {0: 0, 1: 0}\n",
    "        self.class_priors = {}\n",
    "\n",
    "    def fit(self, data, labels):\n",
    "        \"\"\"\n",
    "        Train Naive Bayes by calculating word frequencies for each class.\n",
    "        \"\"\"\n",
    "        for text_vector, label in zip(data, labels):\n",
    "            self.class_totals[label] += np.sum(text_vector)\n",
    "            for i, count in enumerate(text_vector):\n",
    "                self.class_word_counts[label][i] += count\n",
    "        self.class_priors[0] = sum(1 for lbl in labels if lbl == 0) / len(labels)\n",
    "        self.class_priors[1] = sum(1 for lbl in labels if lbl == 1) / len(labels)\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Predict class labels for test data.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for text_vector in data:\n",
    "            log_prob_0 = np.log(self.class_priors[0])\n",
    "            log_prob_1 = np.log(self.class_priors[1])\n",
    "            for i, count in enumerate(text_vector):\n",
    "                log_prob_0 += count * np.log((self.class_word_counts[0][i] + 1) / (self.class_totals[0] + len(self.class_word_counts[0])))\n",
    "                log_prob_1 += count * np.log((self.class_word_counts[1][i] + 1) / (self.class_totals[1] + len(self.class_word_counts[1])))\n",
    "            predictions.append(1 if log_prob_1 > log_prob_0 else 0)\n",
    "        return predictions\n",
    "\n",
    "    def accuracy(self, data, labels):\n",
    "        predictions = self.predict(data)\n",
    "        accuracy = np.mean(np.array(predictions) == np.array(labels))\n",
    "        return accuracy\n",
    "\n",
    "    def confusion_matrix(self, data, labels):\n",
    "        predictions = self.predict(data)\n",
    "        tp = sum(1 for p, l in zip(predictions, labels) if p == l == 1)\n",
    "        tn = sum(1 for p, l in zip(predictions, labels) if p == l == 0)\n",
    "        fp = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)\n",
    "        fn = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)\n",
    "        return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "# Train and Evaluate\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(train_vectors_freq, train_labels)\n",
    "\n",
    "# Evaluate on test data\n",
    "accuracy = nb_classifier.accuracy(test_vectors_freq, test_labels)\n",
    "conf_matrix = nb_classifier.confusion_matrix(test_vectors_freq, test_labels)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Optional: Save results to a log file\n",
    "with open('/Users/saivaruntanjoreraghavendra/Desktop/fall 2024/results.log', 'w') as log_file:\n",
    "    log_file.write(f\"Model Accuracy: {accuracy:.4f}\\n\")\n",
    "    log_file.write(\"Confusion Matrix:\\n\")\n",
    "    log_file.write(str(conf_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f94d10-fe6d-4d05-8410-1cb5d8954777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7944\n",
      "Confusion Matrix:\n",
      "[[2997    3]\n",
      " [ 857  325]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# Initialize the stemmer for stemming version\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def load_data_from_files(directory):\n",
    "    \"\"\"\n",
    "    Load text files from a directory.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            data.append(file.read())\n",
    "    return data\n",
    "\n",
    "def preprocess_text(text, stem=False):\n",
    "    \"\"\"\n",
    "    Preprocesses text by tokenizing, handling punctuation, emoticons, and stemming if specified.\n",
    "    \"\"\"\n",
    "    pattern = r\"\"\"(?x)                  \n",
    "                  (?:[A-Z]\\.)+            # Initials like U.S.A.\n",
    "                  |\\$?\\d+(?:\\.\\d+)?%?     # Currency/Percentages\n",
    "                  |\\w+(?:[-']\\w+)*        # Words with optional internal hyphens/apostrophes\n",
    "                  |[.,;\"'?():-_`]+        # Punctuation\n",
    "                  |[:;=8][\\-o\\*\\']?[)\\](\\d]  # Basic emoticons (like :) or :-))\n",
    "              \"\"\"\n",
    "    tokens = regexp_tokenize(text.lower(), pattern)\n",
    "    tokens = [re.sub(r'<.*?>', '', word) for word in tokens]  # Remove HTML tags\n",
    "    tokens = [stemmer.stem(word) if stem else word for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def build_vocabulary(data, stem=False):\n",
    "    \"\"\"\n",
    "    Build vocabulary from the training data.\n",
    "    \"\"\"\n",
    "    vocabulary = defaultdict(int)\n",
    "    for text in data:\n",
    "        tokens = preprocess_text(text, stem=stem)\n",
    "        for word in tokens:\n",
    "            vocabulary[word] += 1\n",
    "    return vocabulary\n",
    "\n",
    "# Load Data\n",
    "train_positive_data = load_data_from_files('/Users/saivaruntanjoreraghavendra/Downloads/tweet 2/train/positive')\n",
    "train_negative_data = load_data_from_files('/Users/saivaruntanjoreraghavendra/Downloads/tweet 2/train/negative')\n",
    "test_positive_data = load_data_from_files('/Users/saivaruntanjoreraghavendra/Downloads/tweet 2/test/positive')\n",
    "test_negative_data = load_data_from_files('/Users/saivaruntanjoreraghavendra/Downloads/tweet 2/test/negative')\n",
    "\n",
    "# Combine training data\n",
    "train_data = train_positive_data + train_negative_data\n",
    "train_labels = [1] * len(train_positive_data) + [0] * len(train_negative_data)\n",
    "test_data = test_positive_data + test_negative_data\n",
    "test_labels = [1] * len(test_positive_data) + [0] * len(test_negative_data)\n",
    "\n",
    "# Build Vocabulary (With and Without Stemming)\n",
    "vocabulary = build_vocabulary(train_data, stem=False)\n",
    "\n",
    "# Calculate IDF for each term in the vocabulary\n",
    "def compute_idf(data, vocabulary):\n",
    "    \"\"\"\n",
    "    Calculate IDF for each word in the vocabulary.\n",
    "    \"\"\"\n",
    "    doc_count = defaultdict(int)\n",
    "    num_docs = len(data)\n",
    "    \n",
    "    for text in data:\n",
    "        tokens = set(preprocess_text(text))\n",
    "        for token in tokens:\n",
    "            if token in vocabulary:\n",
    "                doc_count[token] += 1\n",
    "\n",
    "    idf = {word: np.log(num_docs / (1 + doc_count[word])) for word in vocabulary}\n",
    "    return idf\n",
    "\n",
    "idf_values = compute_idf(train_data, vocabulary)\n",
    "\n",
    "# Convert Documents to TF-IDF Vectors\n",
    "def document_to_tfidf_vector(text, vocabulary, idf_values, stem=False):\n",
    "    \"\"\"\n",
    "    Convert a document into a TF-IDF vector.\n",
    "    \"\"\"\n",
    "    tokens = preprocess_text(text, stem=stem)\n",
    "    word_counts = Counter(tokens)\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        if word in vocabulary:\n",
    "            tf = count / len(tokens)  # Term Frequency\n",
    "            tf_idf = tf * idf_values[word]  # TF-IDF\n",
    "            index = list(vocabulary.keys()).index(word)\n",
    "            vector[index] = tf_idf\n",
    "    return vector\n",
    "\n",
    "# Convert all training and test data into TF-IDF vectors\n",
    "train_vectors_tfidf = [document_to_tfidf_vector(text, vocabulary, idf_values, stem=False) for text in train_data]\n",
    "test_vectors_tfidf = [document_to_tfidf_vector(text, vocabulary, idf_values, stem=False) for text in test_data]\n",
    "\n",
    "# Naive Bayes Model\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_word_counts = {0: defaultdict(int), 1: defaultdict(int)}\n",
    "        self.class_totals = {0: 0, 1: 0}\n",
    "        self.class_priors = {}\n",
    "\n",
    "    def fit(self, data, labels):\n",
    "        \"\"\"\n",
    "        Train Naive Bayes by calculating word frequencies for each class.\n",
    "        \"\"\"\n",
    "        for text_vector, label in zip(data, labels):\n",
    "            self.class_totals[label] += np.sum(text_vector)\n",
    "            for i, count in enumerate(text_vector):\n",
    "                self.class_word_counts[label][i] += count\n",
    "        self.class_priors[0] = sum(1 for lbl in labels if lbl == 0) / len(labels)\n",
    "        self.class_priors[1] = sum(1 for lbl in labels if lbl == 1) / len(labels)\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Predict class labels for test data.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for text_vector in data:\n",
    "            log_prob_0 = np.log(self.class_priors[0])\n",
    "            log_prob_1 = np.log(self.class_priors[1])\n",
    "            for i, count in enumerate(text_vector):\n",
    "                log_prob_0 += count * np.log((self.class_word_counts[0][i] + 1) / (self.class_totals[0] + len(self.class_word_counts[0])))\n",
    "                log_prob_1 += count * np.log((self.class_word_counts[1][i] + 1) / (self.class_totals[1] + len(self.class_word_counts[1])))\n",
    "            predictions.append(1 if log_prob_1 > log_prob_0 else 0)\n",
    "        return predictions\n",
    "\n",
    "    def accuracy(self, data, labels):\n",
    "        predictions = self.predict(data)\n",
    "        accuracy = np.mean(np.array(predictions) == np.array(labels))\n",
    "        return accuracy\n",
    "\n",
    "    def confusion_matrix(self, data, labels):\n",
    "        predictions = self.predict(data)\n",
    "        tp = sum(1 for p, l in zip(predictions, labels) if p == l == 1)\n",
    "        tn = sum(1 for p, l in zip(predictions, labels) if p == l == 0)\n",
    "        fp = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)\n",
    "        fn = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)\n",
    "        return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "# Train and Evaluate\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(train_vectors_tfidf, train_labels)\n",
    "\n",
    "# Evaluate on test data\n",
    "accuracy = nb_classifier.accuracy(test_vectors_tfidf, test_labels)\n",
    "conf_matrix = nb_classifier.confusion_matrix(test_vectors_tfidf, test_labels)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Optional: Save results to a log file\n",
    "with open('/Users/saivaruntanjoreraghavendra/Desktop/fall 2024/results-tfidf.log', 'w') as log_file:\n",
    "    log_file.write(f\"Model Accuracy: {accuracy:.4f}\\n\")\n",
    "    log_file.write(\"Confusion Matrix:\\n\")\n",
    "    log_file.write(str(conf_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ec72d-fe02-4ccd-a122-48aa21c2b9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
